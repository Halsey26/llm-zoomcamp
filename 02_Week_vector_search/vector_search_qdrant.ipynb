{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01705e21",
   "metadata": {},
   "source": [
    "Vector Search Qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e3ece1",
   "metadata": {},
   "source": [
    "## 1. Importación de librerías y conexión con Qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc86d417",
   "metadata": {},
   "source": [
    "Primero asegúrate de correr e instalar las dependencias necesarias\n",
    "Y dejar el contenedor corriendo. <p>\n",
    "Setup: [set_up_qdrant]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de8b4e1",
   "metadata": {},
   "source": [
    "Importación de librerías\n",
    "- Clase `QdrantClient`: permite establecer conexión con el servicio de Qdrant.\n",
    "- Módulo `models`: permite establecer configuraciones y parámetros necesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2c867a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50d56c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos el cliente\n",
    "client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2ef34d",
   "metadata": {},
   "source": [
    "## 2. Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a0b20",
   "metadata": {},
   "source": [
    "Hay que entender el dataset, conocer principalmente:\n",
    "- Qué tipo de contenido es? imagen, video, texto o combinación\n",
    "- Especificar: Si es texto, entonces qué lenguaje es, contiene caracteres especiales?\n",
    "\n",
    "¿Porqué es necesario responder estas preguntas?\n",
    "Para definir:\n",
    "- El tipo de esquema para los datos. (¿qué es lo se va a vectorizar?, ¿Qué se va a almacenar en la metadata?)\n",
    "- El modelo correcto del embedding. Tenemos que evaluar diferentes parámetros como el dominio, la precisión y los recursos requeridos           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be548526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación del dataset\n",
    "# pip install requests\n",
    "import requests\n",
    "\n",
    "ruta_doc= 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_rpta= requests.get(ruta_doc)\n",
    "documentos = docs_rpta.json()\n",
    "# documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd39bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de documentos:3\n",
      "Data type de documentos:<class 'list'>\n",
      "Data type de dato del primer elemento de documentos:<class 'dict'>\n",
      "\n",
      "Cursos del FaQ Zoomcamp:\n",
      "- data-engineering-zoomcamp\n",
      "- machine-learning-zoomcamp\n",
      "- mlops-zoomcamp\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tamaño de documentos:{len(documentos)}\")\n",
    "print(f\"Data type de documentos:{type(documentos)}\")\n",
    "print(f\"Data type de dato del primer elemento de documentos:{type(documentos[0])}\")\n",
    "\n",
    "print('\\nCursos del FaQ Zoomcamp:')\n",
    "for i in documentos:\n",
    "    print(f\"- {i['course']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b23fcc6",
   "metadata": {},
   "source": [
    "Entonces, después de analizar `documentos`, la información relevante es:\n",
    "- Tamaño: 3 elementos\n",
    "- Lista de diccionarios\n",
    "\n",
    "Estructura de `documentos` <p>\n",
    "\n",
    "```git bash\n",
    "[\n",
    "    {'course':'data-engineering-zoomcamp',\n",
    "    'documents': [{'text': ' ' , 'section':' '  , 'question':' '  },\n",
    "                  {'text': ' ' , 'section':' '  , 'question':' '  },\n",
    "                                 ...                    \n",
    "                {'text':  , 'section':  , 'question':  } ]\n",
    "    \n",
    "    }, \n",
    "    {'course':'machine-learning-zoomcamp',\n",
    "    'documents': [{'text':  , 'section':  , 'question':  },\n",
    "                  {'text':  , 'section':  , 'question':  },\n",
    "                                 ...                    \n",
    "                {'text':  , 'section':  , 'question':  } ]\n",
    "    \n",
    "    }, \n",
    "    {'course':'mlops-zoomcamp',\n",
    "    'documents': [{'text':  , 'section':  , 'question':  },\n",
    "                  {'text':  , 'section':  , 'question':  },\n",
    "                                 ...                    \n",
    "                {'text':  , 'section':  , 'question':  } ]\n",
    "    } \n",
    "]\n",
    "```\n",
    "\n",
    "Cada elemento de la lista es un diccionario, que contiene 2 elementos(course y documents). <p>\n",
    "Del cual documents es también una lista de diccionarios, donde cada diccionario contiene 3 elementos(text, section, question)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcb6bad",
   "metadata": {},
   "source": [
    "1. Verificamos que la data se encuentra limpia y fragmentada (chunk, divide la data en pequeñas partes porque es más fácil para los modelos de embeddings procesarla)\n",
    "2. Tenemos que definir\n",
    "    - Los campos(fields) para semantic_search\n",
    "    - Campos para almacenar como *metadata*, pueden ser usados como filtros "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc265f",
   "metadata": {},
   "source": [
    "3. Se definen: \n",
    "    - Campos para semantic search: question, text\n",
    "    - Campos para metadata(también como filtros): course, section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de69774",
   "metadata": {},
   "source": [
    "## 3. Elección del modelo de embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e8c08",
   "metadata": {},
   "source": [
    "Los modelos de embeddings como ya lo hemos mencionado permite convertir la data en vectores. Para escoger un 'buen' modelo, depende ciertos factores:\n",
    "- La tarea, el tipo de dato y sus características. (texto, inglés )\n",
    "- La evaluación entre la 'precisión de búsqueda' y los 'recursos usados' (embeddings más grandes requieren más almacenamiento y memoria)\n",
    "- El costo de deducir(o inferir) \n",
    "etc\n",
    "\n",
    "La mejor manera de escoger el modelo es testear diferentes opciones en tu propia data. <p>\n",
    "En este caso, vamos a utilizar FastEmbed. <p>\n",
    "Documentación: [FastEmbed](https://github.com/qdrant/fastembed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0178f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'BAAI/bge-base-en',\n",
       "  'sources': {'hf': 'Qdrant/fast-bge-base-en',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.42,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-base-en-v1.5',\n",
       "  'sources': {'hf': 'qdrant/bge-base-en-v1.5-onnx-q',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en-v1.5.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.21,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-large-en-v1.5',\n",
       "  'sources': {'hf': 'qdrant/bge-large-en-v1.5-onnx',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 1.2,\n",
       "  'additional_files': [],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-small-en',\n",
       "  'sources': {'hf': 'Qdrant/bge-small-en',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/BAAI-bge-small-en.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.13,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-small-en-v1.5',\n",
       "  'sources': {'hf': 'qdrant/bge-small-en-v1.5-onnx-q',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.067,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastembed import TextEmbedding\n",
    "\n",
    "# modelos disponibles\n",
    "modelos= TextEmbedding.list_supported_models()\n",
    "\n",
    "# primeros 5 modelos\n",
    "modelos[:5] \n",
    "\n",
    "# modelo_embedding= TextEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9098e017",
   "metadata": {},
   "source": [
    "Es lógico que nuestro modelo no produzca una alta dimensionalidad para evitar usar recursos de más, por lo tanto nos declinamos por una moderada dimensionalidad (Ej: 512 dimensionalidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b876107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4bf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": \"BAAI/bge-small-zh-v1.5\",\n",
      "  \"sources\": {\n",
      "    \"hf\": \"Qdrant/bge-small-zh-v1.5\",\n",
      "    \"url\": \"https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz\",\n",
      "    \"_deprecated_tar_struct\": true\n",
      "  },\n",
      "  \"model_file\": \"model_optimized.onnx\",\n",
      "  \"description\": \"Text embeddings, Unimodal (text), Chinese, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.\",\n",
      "  \"license\": \"mit\",\n",
      "  \"size_in_GB\": 0.09,\n",
      "  \"additional_files\": [],\n",
      "  \"dim\": 512,\n",
      "  \"tasks\": {}\n",
      "}\n",
      "{\n",
      "  \"model\": \"Qdrant/clip-ViT-B-32-text\",\n",
      "  \"sources\": {\n",
      "    \"hf\": \"Qdrant/clip-ViT-B-32-text\",\n",
      "    \"url\": null,\n",
      "    \"_deprecated_tar_struct\": false\n",
      "  },\n",
      "  \"model_file\": \"model.onnx\",\n",
      "  \"description\": \"Text embeddings, Multimodal (text&image), English, 77 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year\",\n",
      "  \"license\": \"mit\",\n",
      "  \"size_in_GB\": 0.25,\n",
      "  \"additional_files\": [],\n",
      "  \"dim\": 512,\n",
      "  \"tasks\": {}\n",
      "}\n",
      "{\n",
      "  \"model\": \"jinaai/jina-embeddings-v2-small-en\",\n",
      "  \"sources\": {\n",
      "    \"hf\": \"xenova/jina-embeddings-v2-small-en\",\n",
      "    \"url\": null,\n",
      "    \"_deprecated_tar_struct\": false\n",
      "  },\n",
      "  \"model_file\": \"onnx/model.onnx\",\n",
      "  \"description\": \"Text embeddings, Unimodal (text), English, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.\",\n",
      "  \"license\": \"apache-2.0\",\n",
      "  \"size_in_GB\": 0.12,\n",
      "  \"additional_files\": [],\n",
      "  \"dim\": 512,\n",
      "  \"tasks\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# para formatear la salida con indentacion\n",
    "import json\n",
    "\n",
    "dimension= 512\n",
    "for i in modelos:\n",
    "    if i['dim'] == dimension:\n",
    "        print (json.dumps(i, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33459847",
   "metadata": {},
   "source": [
    "El modelo 1: `BAAI/bge-small-zh-v1.5`\n",
    "- Para texto pero para lenguaje Chino, descartado\n",
    "\n",
    "El modelo 2: `Qdrant/clip-ViT-B-32-text`\n",
    "- Multimoda, Para texto e imágenes, pero no es necesario impágenes, descartado\n",
    "\n",
    "El modelo 3: `jinaai/jina-embeddings-v2-small-en`\n",
    "- El más adecuado descartando los previos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa0e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_seleccionado= \"jinaai/jina-embeddings-v2-small-en\"\n",
    "\n",
    "# Este modelo como muchos, fue entrenado usando similitud de coseno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo fastembed\n",
    "from fastembed import TextEmbedding\n",
    "\n",
    "\n",
    "# Example list of documents\n",
    "documents: list[str] = [\n",
    "    \"This is built to be faster and lighter than other embedding libraries e.g. Transformers, Sentence-Transformers, etc.\",\n",
    "    \"fastembed is supported by and maintained by Qdrant.\",\n",
    "]\n",
    "\n",
    "# This will trigger the model download and initialization\n",
    "embedding_model = TextEmbedding()\n",
    "print(\"The model BAAI/bge-small-en-v1.5 is ready to use.\")\n",
    "\n",
    "embeddings_generator = embedding_model.embed(documents)  # reminder this is a generator\n",
    "embeddings_list = list(embedding_model.embed(documents))\n",
    "  # you can also convert the generator to a list, and that to a numpy array\n",
    "len(embeddings_list[0]) # Vector of 384 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bef4421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data-engineering-zoomcamp'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "documentos[0]['course']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
